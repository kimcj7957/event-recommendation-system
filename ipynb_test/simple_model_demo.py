#!/usr/bin/env python3
"""
ğŸµ ì´ë²¤íŠ¸ ì¶”ì²œ ì‹œìŠ¤í…œ - ê°„ë‹¨í•œ ëª¨ë¸ í•™ìŠµ ë°ëª¨

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” Jupyter notebookì˜ í•µì‹¬ ë‚´ìš©ì„ Python ìŠ¤í¬ë¦½íŠ¸ë¡œ êµ¬í˜„í•œ ë²„ì „ì…ë‹ˆë‹¤.
"""

import sys
import pandas as pd
import numpy as np
from pathlib import Path
import joblib
import warnings
warnings.filterwarnings('ignore')

# ë¨¸ì‹ ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer
from sklearn.decomposition import TruncatedSVD
from sklearn.neighbors import NearestNeighbors
from scipy.sparse import hstack
import re

def preprocess_text(text):
    """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜"""
    if pd.isna(text):
        return ''
    text = re.sub(r'[^\w\s]', ' ', str(text).lower())
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def load_data():
    """ê¸°ì¡´ ëª¨ë¸ì—ì„œ ë°ì´í„° ë¡œë“œ"""
    model_path = Path('../model/recommender_ko.joblib')
    
    if not model_path.exists():
        print("âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë°±ì—”ë“œë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        return None, None
    
    base_model = joblib.load(model_path)
    df = base_model['df']
    meta_preprocessor = base_model['pre']
    
    print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)}ê°œ ì´ë²¤íŠ¸")
    return df, meta_preprocessor

def build_tfidf_model(df, meta_preprocessor):
    """TF-IDF ëª¨ë¸ êµ¬ì¶•"""
    print("ğŸ”¤ TF-IDF ëª¨ë¸ í•™ìŠµ ì¤‘...")
    
    # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° ì½”í¼ìŠ¤ ìƒì„±
    df['content_clean'] = df['content'].apply(preprocess_text)
    df['place_clean'] = df['place'].apply(preprocess_text)
    df['location_clean'] = df['loc_sigu'].apply(preprocess_text)
    
    text_corpus = (
        df['content_clean'].fillna('') + ' ' +
        df['place_clean'].fillna('') + ' ' +
        df['location_clean'].fillna('')
    )
    
    # TF-IDF ë²¡í„°í™”
    tfidf_vectorizer = TfidfVectorizer(
        max_features=5000,
        ngram_range=(1, 2),
        min_df=2,
        stop_words='english'
    )
    
    X_text = tfidf_vectorizer.fit_transform(text_corpus)
    X_meta = meta_preprocessor.transform(df)
    X_combined = hstack([X_text, X_meta]).tocsr()
    
    # KNN ëª¨ë¸ í•™ìŠµ
    knn = NearestNeighbors(metric='cosine', n_neighbors=10, n_jobs=-1)
    knn.fit(X_combined)
    
    print(f"âœ… TF-IDF ëª¨ë¸ ì™„ë£Œ: {X_combined.shape}")
    
    return {
        'vectorizer': tfidf_vectorizer,
        'knn': knn,
        'text_corpus': text_corpus
    }

def build_lsa_model(df, meta_preprocessor, text_corpus):
    """LSA ëª¨ë¸ êµ¬ì¶•"""
    print("ğŸ§® LSA ëª¨ë¸ í•™ìŠµ ì¤‘...")
    
    # Count Vectorizer
    count_vectorizer = CountVectorizer(
        max_features=3000,
        ngram_range=(1, 2),
        min_df=2,
        stop_words='english'
    )
    
    X_text = count_vectorizer.fit_transform(text_corpus)
    
    # SVD ì°¨ì› ì¶•ì†Œ
    svd = TruncatedSVD(n_components=50, random_state=42)
    X_text_reduced = svd.fit_transform(X_text)
    
    # ë©”íƒ€ë°ì´í„°ì™€ ê²°í•©
    X_meta = meta_preprocessor.transform(df)
    X_meta_dense = X_meta.toarray() if hasattr(X_meta, 'toarray') else X_meta
    X_combined = np.hstack([X_text_reduced, X_meta_dense])
    
    # KNN ëª¨ë¸
    knn = NearestNeighbors(metric='cosine', n_neighbors=10, n_jobs=-1)
    knn.fit(X_combined)
    
    print(f"âœ… LSA ëª¨ë¸ ì™„ë£Œ: {X_combined.shape}")
    
    return {
        'count_vectorizer': count_vectorizer,
        'svd': svd,
        'knn': knn
    }

def test_recommendations(models, df, meta_preprocessor):
    """ì¶”ì²œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ¯ ì¶”ì²œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    test_query = {
        "keywords": "ì¬ì¦ˆ ì½˜ì„œíŠ¸",
        "price_max": 50000,
        "location": "ê°•ë‚¨êµ¬"
    }
    
    print(f"í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: {test_query}")
    
    for model_name, model in models.items():
        print(f"\nğŸ¤– {model_name.upper()} ëª¨ë¸ ê²°ê³¼:")
        print("-" * 30)
        
        try:
            # ì¿¼ë¦¬ ì¸ì½”ë”© (ê°„ë‹¨ ë²„ì „)
            keywords = test_query["keywords"]
            
            if model_name == 'tfidf':
                q_vec = model['vectorizer'].transform([keywords])
                # ë©”íƒ€ë°ì´í„° ê°„ë‹¨íˆ ì²˜ë¦¬
                meta_df = pd.DataFrame([{
                    'price_adv': test_query["price_max"],
                    'price_door': test_query["price_max"],
                    'loc_sigu': test_query["location"]
                }])
                meta_vec = meta_preprocessor.transform(meta_df)
                q_combined = hstack([q_vec, meta_vec])
            
            elif model_name == 'lsa':
                count_vec = model['count_vectorizer'].transform([keywords])
                q_reduced = model['svd'].transform(count_vec)
                meta_df = pd.DataFrame([{
                    'price_adv': test_query["price_max"],
                    'price_door': test_query["price_max"],
                    'loc_sigu': test_query["location"]
                }])
                meta_vec = meta_preprocessor.transform(meta_df)
                meta_vec_dense = meta_vec.toarray() if hasattr(meta_vec, 'toarray') else meta_vec
                q_combined = np.hstack([q_reduced, meta_vec_dense])
            
            # ì¶”ì²œ ì‹¤í–‰
            distances, indices = model['knn'].kneighbors(q_combined, n_neighbors=3)
            similarities = 1 - distances[0]
            
            # ê²°ê³¼ ì¶œë ¥
            for i, (idx, sim) in enumerate(zip(indices[0], similarities), 1):
                event = df.iloc[idx]
                print(f"{i}. ğŸ“ {event['place']} ({event.get('loc_sigu', 'N/A')})")
                print(f"   ğŸ’° {event.get('price_adv', 0):,.0f}ì›")
                print(f"   ğŸ“Š ìœ ì‚¬ë„: {sim:.3f}")
                print(f"   ğŸ“ {event['content'][:60]}...")
                print()
                
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜: {e}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸµ ì´ë²¤íŠ¸ ì¶”ì²œ ì‹œìŠ¤í…œ - ëª¨ë¸ í•™ìŠµ ë°ëª¨")
    print("=" * 60)
    
    # 1. ë°ì´í„° ë¡œë“œ
    df, meta_preprocessor = load_data()
    if df is None:
        return
    
    print(f"\nğŸ“Š ë°ì´í„° ì •ë³´:")
    print(f"â€¢ ì´ë²¤íŠ¸ ìˆ˜: {len(df):,}ê°œ")
    print(f"â€¢ ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}ê°œ")
    print(f"â€¢ ì£¼ìš” ì»¬ëŸ¼: {list(df.columns[:5])}")
    
    # 2. í…ìŠ¤íŠ¸ ì½”í¼ìŠ¤ ìƒì„±
    print(f"\nğŸ”§ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì¤‘...")
    df['content_clean'] = df['content'].apply(preprocess_text)
    df['place_clean'] = df['place'].apply(preprocess_text)
    df['location_clean'] = df['loc_sigu'].apply(preprocess_text)
    
    text_corpus = (
        df['content_clean'].fillna('') + ' ' +
        df['place_clean'].fillna('') + ' ' +
        df['location_clean'].fillna('')
    )
    
    # 3. ëª¨ë¸ í•™ìŠµ
    models = {}
    
    # TF-IDF ëª¨ë¸
    models['tfidf'] = build_tfidf_model(df, meta_preprocessor)
    
    # LSA ëª¨ë¸
    models['lsa'] = build_lsa_model(df, meta_preprocessor, text_corpus)
    
    print(f"\nâœ… ì´ {len(models)}ê°œ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
    
    # 4. ì¶”ì²œ í…ŒìŠ¤íŠ¸
    test_recommendations(models, df, meta_preprocessor)
    
    print("\nğŸ‰ ë°ëª¨ ì™„ë£Œ!")
    print("ğŸ’¡ ë” ìì„¸í•œ ë‚´ìš©ì€ model_training_demo.ipynb ë…¸íŠ¸ë¶ì„ í™•ì¸í•˜ì„¸ìš”.")

if __name__ == "__main__":
    main() 