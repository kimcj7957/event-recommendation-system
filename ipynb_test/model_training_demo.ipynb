{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# ğŸµ ì´ë²¤íŠ¸ ì¶”ì²œ ì‹œìŠ¤í…œ - ì™„ì „í•œ ëª¨ë¸ í•™ìŠµ ë°ëª¨\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ ë‹¤ì¤‘ ëª¨ë¸ ê¸°ë°˜ ì´ë²¤íŠ¸ ì¶”ì²œ ì‹œìŠ¤í…œì˜ ì „ì²´ í•™ìŠµ ê³¼ì •ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n",
    "\n",
    "## ğŸ“‹ ëª©ì°¨\n",
    "1. í™˜ê²½ ì„¤ì • ë° ë°ì´í„° ë¡œë“œ\n",
    "2. ë°ì´í„° ì „ì²˜ë¦¬ ë° í•„í„°ë§\n",
    "3. TF-IDF ëª¨ë¸ í•™ìŠµ\n",
    "4. LSA ëª¨ë¸ í•™ìŠµ\n",
    "5. Word2Vec ëŒ€ì•ˆ ëª¨ë¸ í•™ìŠµ\n",
    "6. í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ êµ¬ì¶•\n",
    "7. ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ\n",
    "8. ì¶”ì²œ ê²°ê³¼ ì‹œì—°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì„¤ì • ë° ë°ì´í„° ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ë¨¸ì‹ ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import hstack\n",
    "import re\n",
    "\n",
    "# í”Œë¡¯ ì„¤ì •\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"ğŸš€ ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê¸°ì¡´ ëª¨ë¸ì—ì„œ ë°ì´í„° ë¡œë“œ\n",
    "model_path = Path('../model/recommender_ko.joblib')\n",
    "\n",
    "if model_path.exists():\n",
    "    base_model = joblib.load(model_path)\n",
    "    df_original = base_model['df']\n",
    "    meta_preprocessor = base_model['pre']\n",
    "    print(f\"âœ… ì›ë³¸ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df_original):,}ê°œ ì´ë²¤íŠ¸\")\n",
    "    print(f\"ğŸ“ˆ ë°ì´í„°ì…‹ í¬ê¸°: {df_original.shape}\")\n",
    "    print(f\"ğŸ“‹ ì»¬ëŸ¼: {list(df_original.columns)}\")\n",
    "else:\n",
    "    print(\"âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë°±ì—”ë“œë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.\")\n",
    "    df_original = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 2. ë°ì´í„° ì „ì²˜ë¦¬ ë° í•„í„°ë§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
    "def preprocess_text(text):\n",
    "    \"\"\"í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return ''\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', str(text).lower())\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def filter_valid_events(df):\n",
    "    \"\"\"ìœ íš¨í•œ ì´ë²¤íŠ¸ ë°ì´í„°ë§Œ í•„í„°ë§\"\"\"\n",
    "    print(\"ğŸ”§ ë°ì´í„° í•„í„°ë§ ë° ì „ì²˜ë¦¬ ì¤‘...\")\n",
    "    print(f\"ì›ë³¸ ë°ì´í„°: {len(df):,}ê°œ ì´ë²¤íŠ¸\")\n",
    "    \n",
    "    # ê¸°ë³¸ ì „ì²˜ë¦¬\n",
    "    df_filtered = df.copy()\n",
    "    df_filtered['content_clean'] = df_filtered['content'].apply(preprocess_text)\n",
    "    df_filtered['place_clean'] = df_filtered['place'].apply(preprocess_text)\n",
    "    df_filtered['location_clean'] = df_filtered['loc_sigu'].apply(preprocess_text)\n",
    "    \n",
    "    # í•„í„°ë§ ì¡°ê±´\n",
    "    # 1. contentê°€ ë¹„ì–´ìˆì§€ ì•Šê³  10ì ì´ìƒ\n",
    "    valid_content = (df_filtered['content'].notna()) & (df_filtered['content'].str.len() >= 10)\n",
    "    print(f\"ìœ íš¨í•œ ë‚´ìš©(10ì ì´ìƒ): {valid_content.sum():,}ê°œ\")\n",
    "    \n",
    "    # 2. place ì •ë³´ê°€ ìˆëŠ” ê²ƒ\n",
    "    valid_place = df_filtered['place'].notna() & (df_filtered['place'].str.len() > 0)\n",
    "    print(f\"ìœ íš¨í•œ ì¥ì†Œ ì •ë³´: {valid_place.sum():,}ê°œ\")\n",
    "    \n",
    "    # 3. ê°€ê²© ì •ë³´ê°€ ìˆëŠ” ê²ƒ (0ë³´ë‹¤ í° ê°’)\n",
    "    valid_price = df_filtered['price_adv'].notna() & (df_filtered['price_adv'] > 0)\n",
    "    print(f\"ìœ íš¨í•œ ê°€ê²© ì •ë³´: {valid_price.sum():,}ê°œ\")\n",
    "    \n",
    "    # ëª¨ë“  ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ë°ì´í„°ë§Œ ì„ íƒ\n",
    "    valid_mask = valid_content & valid_place & valid_price\n",
    "    df_filtered = df_filtered[valid_mask].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"âœ… í•„í„°ë§ ì™„ë£Œ: {len(df_filtered):,}ê°œ ì´ë²¤íŠ¸ (ì œê±°ëœ ê²ƒ: {len(df) - len(df_filtered):,}ê°œ)\")\n",
    "    print(f\"ğŸ“Š í•„í„°ë§ ë¹„ìœ¨: {len(df_filtered)/len(df)*100:.1f}%\")\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "if df_original is not None:\n",
    "    # ë°ì´í„° í•„í„°ë§ ì ìš©\n",
    "    df = filter_valid_events(df_original)\n",
    "    \n",
    "    # í†µí•© í…ìŠ¤íŠ¸ ì½”í¼ìŠ¤ ìƒì„±\n",
    "    text_corpus = (\n",
    "        df['content_clean'].fillna('') + ' ' +\n",
    "        df['place_clean'].fillna('') + ' ' +\n",
    "        df['location_clean'].fillna('')\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nğŸ“ í…ìŠ¤íŠ¸ ì½”í¼ìŠ¤ í†µê³„:\")\n",
    "    print(f\"â€¢ í‰ê·  ê¸¸ì´: {text_corpus.str.len().mean():.0f}ì\")\n",
    "    print(f\"â€¢ ìµœëŒ€ ê¸¸ì´: {text_corpus.str.len().max():.0f}ì\")\n",
    "    print(f\"â€¢ ìµœì†Œ ê¸¸ì´: {text_corpus.str.len().min():.0f}ì\")\n",
    "    \n",
    "    # ìƒ˜í”Œ ë°ì´í„° í™•ì¸\n",
    "    print(\"\\nğŸ” í•„í„°ë§ëœ ë°ì´í„° ìƒ˜í”Œ:\")\n",
    "    display(df[['content', 'place', 'price_adv', 'loc_sigu']].head(3))\n",
    "else:\n",
    "    df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## 3. TF-IDF ëª¨ë¸ í•™ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(\"ğŸ”¤ TF-IDF ëª¨ë¸ í•™ìŠµ ì‹œì‘...\")\n",
    "    \n",
    "    # TF-IDF ë²¡í„°ë¼ì´ì € ì„¤ì •\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        max_features=10000,\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=2,\n",
    "        stop_words='english'\n",
    "    )\n",
    "    \n",
    "    # í…ìŠ¤íŠ¸ ë²¡í„°í™”\n",
    "    X_text_tfidf = tfidf_vectorizer.fit_transform(text_corpus)\n",
    "    print(f\"âœ… TF-IDF ë²¡í„° ìƒì„±: {X_text_tfidf.shape}\")\n",
    "    \n",
    "    # ë©”íƒ€ë°ì´í„°ì™€ ê²°í•©\n",
    "    X_meta = meta_preprocessor.transform(df)\n",
    "    X_combined_tfidf = hstack([X_text_tfidf, X_meta]).tocsr()\n",
    "    print(f\"âœ… í†µí•© íŠ¹ì„± ë²¡í„°: {X_combined_tfidf.shape}\")\n",
    "    \n",
    "    # KNN ëª¨ë¸ í•™ìŠµ\n",
    "    knn_tfidf = NearestNeighbors(metric='cosine', n_neighbors=20, n_jobs=-1)\n",
    "    knn_tfidf.fit(X_combined_tfidf)\n",
    "    print(\"âœ… TF-IDF KNN ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\")\n",
    "    \n",
    "    # ìƒìœ„ íŠ¹ì„± ë¶„ì„\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    tfidf_scores = X_text_tfidf.mean(axis=0).A1\n",
    "    top_features = sorted(zip(feature_names, tfidf_scores), key=lambda x: x[1], reverse=True)[:10]\n",
    "    \n",
    "    print(\"\\nğŸ† TF-IDF ìƒìœ„ 10ê°œ íŠ¹ì„±:\")\n",
    "    for i, (feature, score) in enumerate(top_features, 1):\n",
    "        print(f\"{i:2d}. {feature:<15} (ì ìˆ˜: {score:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 4. LSA (ì ì¬ ì˜ë¯¸ ë¶„ì„) ëª¨ë¸ í•™ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(\"ğŸ§® LSA ëª¨ë¸ í•™ìŠµ ì‹œì‘...\")\n",
    "    \n",
    "    # Count Vectorizer (LSAì—ì„œ ë” íš¨ê³¼ì )\n",
    "    count_vectorizer = CountVectorizer(\n",
    "        max_features=5000,\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=2,\n",
    "        stop_words='english'\n",
    "    )\n",
    "    \n",
    "    X_text_count = count_vectorizer.fit_transform(text_corpus)\n",
    "    print(f\"âœ… Count ë²¡í„° ìƒì„±: {X_text_count.shape}\")\n",
    "    \n",
    "    # SVDë¡œ ì°¨ì› ì¶•ì†Œ\n",
    "    svd = TruncatedSVD(n_components=100, random_state=42)\n",
    "    X_text_lsa = svd.fit_transform(X_text_count)\n",
    "    print(f\"âœ… LSA ì°¨ì› ì¶•ì†Œ: {X_text_lsa.shape}\")\n",
    "    print(f\"âœ… ì„¤ëª…ëœ ë¶„ì‚° ë¹„ìœ¨: {svd.explained_variance_ratio_.sum():.3f}\")\n",
    "    \n",
    "    # ë©”íƒ€ë°ì´í„°ì™€ ê²°í•©\n",
    "    X_meta_dense = X_meta.toarray() if hasattr(X_meta, 'toarray') else X_meta\n",
    "    X_combined_lsa = np.hstack([X_text_lsa, X_meta_dense])\n",
    "    print(f\"âœ… í†µí•© LSA íŠ¹ì„± ë²¡í„°: {X_combined_lsa.shape}\")\n",
    "    \n",
    "    # KNN ëª¨ë¸ í•™ìŠµ\n",
    "    knn_lsa = NearestNeighbors(metric='cosine', n_neighbors=20, n_jobs=-1)\n",
    "    knn_lsa.fit(X_combined_lsa)\n",
    "    print(\"âœ… LSA KNN ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\")\n",
    "    \n",
    "    # LSA ì„±ë¶„ ì‹œê°í™”\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, 101), svd.explained_variance_ratio_, 'b-', alpha=0.7)\n",
    "    plt.title('LSA ì„±ë¶„ë³„ ì„¤ëª… ë¶„ì‚°')\n",
    "    plt.xlabel('ì„±ë¶„ ë²ˆí˜¸')\n",
    "    plt.ylabel('ì„¤ëª… ë¶„ì‚° ë¹„ìœ¨')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## 5. Word2Vec ëŒ€ì•ˆ ëª¨ë¸ í•™ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(\"ğŸ”¤ Word2Vec ëŒ€ì•ˆ ëª¨ë¸ í•™ìŠµ ì‹œì‘...\")\n",
    "    print(\"(HashingVectorizerë¥¼ ì‚¬ìš©í•˜ì—¬ Word2Vecê³¼ ìœ ì‚¬í•œ íš¨ê³¼ êµ¬í˜„)\")\n",
    "    \n",
    "    # HashingVectorizerë¡œ Word2Vec ëŒ€ì•ˆ êµ¬í˜„\n",
    "    word2vec_hasher = HashingVectorizer(\n",
    "        n_features=1000,\n",
    "        ngram_range=(1, 3),\n",
    "        binary=False,\n",
    "        norm='l2',\n",
    "        lowercase=True,\n",
    "        stop_words='english'\n",
    "    )\n",
    "    \n",
    "    X_text_w2v = word2vec_hasher.fit_transform(text_corpus)\n",
    "    print(f\"âœ… Word2Vec ëŒ€ì•ˆ ë²¡í„° ìƒì„±: {X_text_w2v.shape}\")\n",
    "    \n",
    "    # ë©”íƒ€ë°ì´í„°ì™€ ê²°í•©\n",
    "    X_combined_w2v = hstack([X_text_w2v, X_meta]).tocsr()\n",
    "    print(f\"âœ… í†µí•© Word2Vec íŠ¹ì„± ë²¡í„°: {X_combined_w2v.shape}\")\n",
    "    \n",
    "    # KNN ëª¨ë¸ í•™ìŠµ\n",
    "    knn_w2v = NearestNeighbors(metric='cosine', n_neighbors=20, n_jobs=-1)\n",
    "    knn_w2v.fit(X_combined_w2v)\n",
    "    print(\"âœ… Word2Vec ëŒ€ì•ˆ KNN ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\")\n",
    "    \n",
    "    # í•´ì‹± ë²¡í„° í†µê³„\n",
    "    hash_density = X_text_w2v.nnz / (X_text_w2v.shape[0] * X_text_w2v.shape[1])\n",
    "    print(f\"\\nğŸ“Š HashingVectorizer í†µê³„:\")\n",
    "    print(f\"â€¢ ë²¡í„° ë°€ë„: {hash_density:.4f}\")\n",
    "    print(f\"â€¢ í‰ê·  ë¹„ì˜ ì›ì†Œ ìˆ˜: {X_text_w2v.nnz / X_text_w2v.shape[0]:.1f}\")\n",
    "    print(f\"â€¢ íŠ¹ì„± ì°¨ì›: {X_text_w2v.shape[1]:,}ê°œ\")\n",
    "    \n",
    "    # Word2Vec ëŒ€ì•ˆì˜ ì¥ì  ì„¤ëª…\n",
    "    print(\"\\nğŸ’¡ Word2Vec ëŒ€ì•ˆì˜ íŠ¹ì§•:\")\n",
    "    print(\"â€¢ HashingVectorizerëŠ” ë‹¨ì–´ì˜ í•´ì‹œê°’ì„ ì´ìš©í•˜ì—¬ ê³ ì • í¬ê¸° ë²¡í„° ìƒì„±\")\n",
    "    print(\"â€¢ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì´ë©° ìƒˆë¡œìš´ ë‹¨ì–´ì—ë„ ëŒ€ì‘ ê°€ëŠ¥\")\n",
    "    print(\"â€¢ n-gramì„ í†µí•´ ë‹¨ì–´ ìˆœì„œì™€ ë¬¸ë§¥ ì •ë³´ ì¼ë¶€ ë³´ì¡´\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## 6. í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ êµ¬ì¶•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(\"ğŸ”„ í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ êµ¬ì¶•...\")\n",
    "    \n",
    "    # ëª¨ë“  ëª¨ë¸ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ì •ë¦¬\n",
    "    models = {\n",
    "        'tfidf': {\n",
    "            'vectorizer': tfidf_vectorizer,\n",
    "            'knn': knn_tfidf,\n",
    "            'feature_matrix': X_combined_tfidf,\n",
    "            'description': 'TF-IDF ê¸°ë°˜ í‚¤ì›Œë“œ ë§¤ì¹­',\n",
    "            'type': 'sparse'\n",
    "        },\n",
    "        'lsa': {\n",
    "            'count_vectorizer': count_vectorizer,\n",
    "            'svd': svd,\n",
    "            'knn': knn_lsa,\n",
    "            'feature_matrix': X_combined_lsa,\n",
    "            'description': 'LSA ê¸°ë°˜ ì ì¬ ì˜ë¯¸ ë¶„ì„',\n",
    "            'type': 'dense'\n",
    "        },\n",
    "        'word2vec': {\n",
    "            'hasher': word2vec_hasher,\n",
    "            'knn': knn_w2v,\n",
    "            'feature_matrix': X_combined_w2v,\n",
    "            'description': 'HashingVectorizer ê¸°ë°˜ ë‹¨ì–´ ì„ë² ë”© ëŒ€ì•ˆ',\n",
    "            'type': 'sparse'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"âœ… í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ!\")\n",
    "    print(\"\\nğŸ¯ í•™ìŠµëœ ëª¨ë¸ë“¤:\")\n",
    "    for name, model in models.items():\n",
    "        matrix_shape = model['feature_matrix'].shape\n",
    "        print(f\"â€¢ {name.upper()}: {model['description']}\")\n",
    "        print(f\"  - íŠ¹ì„± í–‰ë ¬ í¬ê¸°: {matrix_shape}\")\n",
    "        print(f\"  - ë°ì´í„° íƒ€ì…: {model['type']}\")\n",
    "        print()\n",
    "    \n",
    "    # í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ í•¨ìˆ˜ ì •ì˜\n",
    "    def hybrid_recommend(query, top_k=5, weights=None):\n",
    "        \"\"\"í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ: ì—¬ëŸ¬ ëª¨ë¸ì˜ ê²°ê³¼ë¥¼ ê°€ì¤‘ í‰ê· \"\"\"\n",
    "        if weights is None:\n",
    "            weights = {'tfidf': 0.4, 'lsa': 0.3, 'word2vec': 0.3}\n",
    "        \n",
    "        all_results = []\n",
    "        \n",
    "        for model_name, weight in weights.items():\n",
    "            try:\n",
    "                q_vec = encode_query_for_model(query, model_name)\n",
    "                distances, indices = models[model_name]['knn'].kneighbors(q_vec, n_neighbors=top_k*2)\n",
    "                \n",
    "                for idx, dist in zip(indices[0], distances[0]):\n",
    "                    similarity = (1 - dist) * weight\n",
    "                    all_results.append((idx, similarity, model_name))\n",
    "            except Exception as e:\n",
    "                print(f\"Error in {model_name}: {e}\")\n",
    "        \n",
    "        # ê²°ê³¼ ì§‘ê³„ ë° ì •ë ¬\n",
    "        result_dict = {}\n",
    "        for idx, sim, model in all_results:\n",
    "            if idx not in result_dict:\n",
    "                result_dict[idx] = {'total_similarity': 0, 'models': []}\n",
    "            result_dict[idx]['total_similarity'] += sim\n",
    "            result_dict[idx]['models'].append(model)\n",
    "        \n",
    "        # ìƒìœ„ kê°œ ì„ íƒ\n",
    "        sorted_results = sorted(result_dict.items(), \n",
    "                               key=lambda x: x[1]['total_similarity'], \n",
    "                               reverse=True)[:top_k]\n",
    "        \n",
    "        return sorted_results\n",
    "    \n",
    "    print(\"ğŸ’¡ í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ íŠ¹ì§•:\")\n",
    "    print(\"â€¢ TF-IDF: ì •í™•í•œ í‚¤ì›Œë“œ ë§¤ì¹­ (ê°€ì¤‘ì¹˜ 40%)\")\n",
    "    print(\"â€¢ LSA: ì˜ë¯¸ì  ìœ ì‚¬ì„± ë¶„ì„ (ê°€ì¤‘ì¹˜ 30%)\")\n",
    "    print(\"â€¢ Word2Vec ëŒ€ì•ˆ: ë‹¨ì–´ ì„ë² ë”© íš¨ê³¼ (ê°€ì¤‘ì¹˜ 30%)\")\n",
    "    print(\"â€¢ ê° ëª¨ë¸ì˜ ê°•ì ì„ ê²°í•©í•˜ì—¬ ë” ë‹¤ì–‘í•˜ê³  ì •í™•í•œ ì¶”ì²œ ì œê³µ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## 7. ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # ì¿¼ë¦¬ ì¸ì½”ë”© í•¨ìˆ˜\n",
    "    def encode_query_for_model(query, model_name):\n",
    "        \"\"\"ì¿¼ë¦¬ë¥¼ íŠ¹ì • ëª¨ë¸ìš©ìœ¼ë¡œ ì¸ì½”ë”©\"\"\"\n",
    "        keywords = query.get('keywords', '')\n",
    "        price_max = query.get('price_max', 25000)\n",
    "        location = query.get('location', 'unknown')\n",
    "        \n",
    "        # ë©”íƒ€ë°ì´í„° ì²˜ë¦¬\n",
    "        meta_df = pd.DataFrame([{\n",
    "            'price_adv': price_max,\n",
    "            'price_door': price_max,\n",
    "            'loc_sigu': location\n",
    "        }])\n",
    "        meta_vec = meta_preprocessor.transform(meta_df)\n",
    "        \n",
    "        model = models[model_name]\n",
    "        \n",
    "        if model_name == 'tfidf':\n",
    "            text_vec = model['vectorizer'].transform([keywords])\n",
    "            return hstack([text_vec, meta_vec])\n",
    "        elif model_name == 'lsa':\n",
    "            count_vec = model['count_vectorizer'].transform([keywords])\n",
    "            text_reduced = model['svd'].transform(count_vec)\n",
    "            meta_vec_dense = meta_vec.toarray() if hasattr(meta_vec, 'toarray') else meta_vec\n",
    "            return np.hstack([text_reduced, meta_vec_dense])\n",
    "        elif model_name == 'word2vec':\n",
    "            text_vec = model['hasher'].transform([keywords])\n",
    "            return hstack([text_vec, meta_vec])\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì •ì˜\n",
    "    test_queries = [\n",
    "        {\"keywords\": \"ì¬ì¦ˆ ì½˜ì„œíŠ¸\", \"price_max\": 50000, \"location\": \"ê°•ë‚¨êµ¬\"},\n",
    "        {\"keywords\": \"í´ë˜ì‹ ìŒì•…íšŒ\", \"price_max\": 30000, \"location\": \"ì¢…ë¡œêµ¬\"},\n",
    "        {\"keywords\": \"ë¡ í˜ìŠ¤í‹°ë²Œ\", \"price_max\": 80000, \"location\": \"ë§ˆí¬êµ¬\"},\n",
    "        {\"keywords\": \"íŒ ê³µì—°\", \"price_max\": 40000, \"location\": \"ì„œì´ˆêµ¬\"}\n",
    "    ]\n",
    "    \n",
    "    print(\"ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # ê° ëª¨ë¸ì˜ ì„±ëŠ¥ ì¸¡ì •\n",
    "    performance_results = {}\n",
    "    \n",
    "    for model_name in models.keys():\n",
    "        print(f\"\\nğŸ” {model_name.upper()} ëª¨ë¸ í…ŒìŠ¤íŠ¸:\")\n",
    "        model_results = []\n",
    "        \n",
    "        for i, query in enumerate(test_queries, 1):\n",
    "            try:\n",
    "                q_vec = encode_query_for_model(query, model_name)\n",
    "                distances, indices = models[model_name]['knn'].kneighbors(q_vec, n_neighbors=5)\n",
    "                similarities = 1 - distances[0]\n",
    "                avg_similarity = similarities.mean()\n",
    "                model_results.append(avg_similarity)\n",
    "                print(f\"  ì¿¼ë¦¬ {i}: '{query['keywords']}' â†’ í‰ê·  ìœ ì‚¬ë„: {avg_similarity:.3f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ì¿¼ë¦¬ {i} ì˜¤ë¥˜: {e}\")\n",
    "                model_results.append(0)\n",
    "        \n",
    "        performance_results[model_name] = model_results\n",
    "    \n",
    "    # ì„±ëŠ¥ ìš”ì•½\n",
    "    print(\"\\nğŸ“ˆ ëª¨ë¸ë³„ í‰ê·  ì„±ëŠ¥:\")\n",
    "    print(\"-\" * 40)\n",
    "    for model_name, results in performance_results.items():\n",
    "        avg_performance = np.mean(results)\n",
    "        print(f\"â€¢ {model_name.upper():<10}: {avg_performance:.3f}\")\n",
    "    \n",
    "    # ì„±ëŠ¥ ì‹œê°í™”\n",
    "    if len(performance_results) > 0:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # ì¿¼ë¦¬ë³„ ì„±ëŠ¥ ë¹„êµ\n",
    "        plt.subplot(1, 2, 1)\n",
    "        x = np.arange(len(test_queries))\n",
    "        width = 0.25\n",
    "        \n",
    "        for i, (model_name, results) in enumerate(performance_results.items()):\n",
    "            plt.bar(x + i*width, results, width, label=model_name.upper(), alpha=0.8)\n",
    "        \n",
    "        plt.xlabel('í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬')\n",
    "        plt.ylabel('í‰ê·  ì½”ì‚¬ì¸ ìœ ì‚¬ë„')\n",
    "        plt.title('ì¿¼ë¦¬ë³„ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ')\n",
    "        plt.xticks(x + width, [f\"Q{i+1}\" for i in range(len(test_queries))])\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # ëª¨ë¸ë³„ í‰ê·  ì„±ëŠ¥\n",
    "        plt.subplot(1, 2, 2)\n",
    "        model_names = list(performance_results.keys())\n",
    "        avg_scores = [np.mean(performance_results[name]) for name in model_names]\n",
    "        \n",
    "        bars = plt.bar(model_names, avg_scores, color=['skyblue', 'lightgreen', 'orange'], alpha=0.8)\n",
    "        plt.ylabel('í‰ê·  ìœ ì‚¬ë„')\n",
    "        plt.title('ëª¨ë¸ë³„ ì „ì²´ í‰ê·  ì„±ëŠ¥')\n",
    "        plt.ylim(0, max(avg_scores) * 1.1)\n",
    "        \n",
    "        # ë§‰ëŒ€ ìœ„ì— ìˆ˜ì¹˜ í‘œì‹œ\n",
    "        for bar, score in zip(bars, avg_scores):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                    f'{score:.3f}', ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## 8. ì¶”ì²œ ê²°ê³¼ ì‹œì—°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    def get_recommendations(query, model_name, top_k=3):\n",
    "        \"\"\"íŠ¹ì • ëª¨ë¸ë¡œ ì¶”ì²œ ê²°ê³¼ ë°˜í™˜\"\"\"\n",
    "        try:\n",
    "            q_vec = encode_query_for_model(query, model_name)\n",
    "            distances, indices = models[model_name]['knn'].kneighbors(q_vec, n_neighbors=top_k)\n",
    "            \n",
    "            results = df.iloc[indices[0]].copy()\n",
    "            results['similarity'] = 1 - distances[0]\n",
    "            \n",
    "            return results[['content', 'place', 'price_adv', 'loc_sigu', 'similarity']]\n",
    "        except Exception as e:\n",
    "            print(f\"ì¶”ì²œ ì˜¤ë¥˜ ({model_name}): {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    # ì‹œì—°ìš© ì¿¼ë¦¬ë“¤\n",
    "    demo_queries = [\n",
    "        {\n",
    "            \"keywords\": \"ì¬ì¦ˆ ì½˜ì„œíŠ¸ ë¼ì´ë¸Œ\",\n",
    "            \"price_max\": 60000,\n",
    "            \"location\": \"ê°•ë‚¨êµ¬\"\n",
    "        },\n",
    "        {\n",
    "            \"keywords\": \"í´ë˜ì‹ ì˜¤ì¼€ìŠ¤íŠ¸ë¼\",\n",
    "            \"price_max\": 50000,\n",
    "            \"location\": \"ì¢…ë¡œêµ¬\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for query_idx, demo_query in enumerate(demo_queries, 1):\n",
    "        print(f\"\\nğŸ¯ ë°ëª¨ ì¿¼ë¦¬ {query_idx}: {demo_query}\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # ê° ëª¨ë¸ë³„ ì¶”ì²œ ê²°ê³¼ ë¹„êµ\n",
    "        for model_name in models.keys():\n",
    "            print(f\"\\nğŸ¤– {model_name.upper()} ëª¨ë¸ ì¶”ì²œ ê²°ê³¼:\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            recommendations = get_recommendations(demo_query, model_name, top_k=3)\n",
    "            \n",
    "            if not recommendations.empty:\n",
    "                for i, (idx, row) in enumerate(recommendations.iterrows(), 1):\n",
    "                    print(f\"{i}. ğŸ“ {row['place'][:40]}... ({row['loc_sigu']})\")\n",
    "                    print(f\"   ğŸ’° ê°€ê²©: {row['price_adv']:,.0f}ì›\")\n",
    "                    print(f\"   ğŸ“Š ìœ ì‚¬ë„: {row['similarity']:.3f}\")\n",
    "                    print(f\"   ğŸ“ ë‚´ìš©: {row['content'][:80]}...\")\n",
    "                    print()\n",
    "            else:\n",
    "                print(\"âŒ ì¶”ì²œ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        \n",
    "        # í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ ì‹œì—°\n",
    "        print(f\"\\nğŸ”„ HYBRID ëª¨ë¸ ì¶”ì²œ ê²°ê³¼ (ê°€ì¤‘ í‰ê· ):\")\n",
    "        print(\"-\" * 50)\n",
    "        try:\n",
    "            hybrid_results = hybrid_recommend(demo_query, top_k=3)\n",
    "            for i, (idx, result_info) in enumerate(hybrid_results, 1):\n",
    "                event = df.iloc[idx]\n",
    "                print(f\"{i}. ğŸ“ {event['place'][:40]}... ({event['loc_sigu']})\")\n",
    "                print(f\"   ğŸ’° ê°€ê²©: {event['price_adv']:,.0f}ì›\")\n",
    "                print(f\"   ğŸ“Š ì¢…í•© ì ìˆ˜: {result_info['total_similarity']:.3f}\")\n",
    "                print(f\"   ğŸ¤ ì°¸ì—¬ ëª¨ë¸: {', '.join(result_info['models'])}\")\n",
    "                print(f\"   ğŸ“ ë‚´ìš©: {event['content'][:80]}...\")\n",
    "                print()\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ ì˜¤ë¥˜: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## ğŸ‰ í•™ìŠµ ì™„ë£Œ!\n",
    "\n",
    "ì¶•í•˜í•©ë‹ˆë‹¤! ë‹¤ì¤‘ ëª¨ë¸ ì´ë²¤íŠ¸ ì¶”ì²œ ì‹œìŠ¤í…œì˜ ì „ì²´ í•™ìŠµ ê³¼ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "### âœ… êµ¬í˜„ëœ ê¸°ëŠ¥ë“¤:\n",
    "\n",
    "#### 1. ë°ì´í„° ì „ì²˜ë¦¬ ë° í•„í„°ë§\n",
    "- ë³¸ë¬¸ì´ 10ì ì´í•˜ì¸ ì´ë²¤íŠ¸ ì œì™¸\n",
    "- ì¥ì†Œ, ê°€ê²© ì •ë³´ê°€ ì—†ëŠ” ì´ë²¤íŠ¸ ì œì™¸\n",
    "- í…ìŠ¤íŠ¸ ì •ê·œí™” ë° ì •ì œ\n",
    "\n",
    "#### 2. í•™ìŠµëœ ëª¨ë¸ë“¤\n",
    "- **TF-IDF ëª¨ë¸**: í‚¤ì›Œë“œ ê¸°ë°˜ ì •í™•í•œ ë§¤ì¹­\n",
    "- **LSA ëª¨ë¸**: ì ì¬ ì˜ë¯¸ ë¶„ì„ìœ¼ë¡œ ì˜ë¯¸ì  ìœ ì‚¬ì„± ê³ ë ¤  \n",
    "- **Word2Vec ëŒ€ì•ˆ**: HashingVectorizerë¡œ ë‹¨ì–´ ì„ë² ë”© íš¨ê³¼\n",
    "- **í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸**: ì„¸ ëª¨ë¸ì˜ ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ì¢…í•©ì  ì¶”ì²œ\n",
    "\n",
    "#### 3. ì„±ëŠ¥ í‰ê°€\n",
    "- ë‹¤ì–‘í•œ ì¿¼ë¦¬ë¡œ ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ\n",
    "- ì‹œê°í™”ë¥¼ í†µí•œ ì„±ëŠ¥ ë¶„ì„\n",
    "- ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ ì •ëŸ‰ì  í‰ê°€\n",
    "\n",
    "#### 4. ì‹¤ì œ ì¶”ì²œ ì‹œì—°\n",
    "- ê° ëª¨ë¸ë³„ ì¶”ì²œ ê²°ê³¼ ë¹„êµ\n",
    "- í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ì˜ ì¢…í•©ì  ì¶”ì²œ\n",
    "- ìœ ì‚¬ë„ ì ìˆ˜ ë° ì°¸ì—¬ ëª¨ë¸ í‘œì‹œ\n",
    "\n",
    "### ğŸš€ ë‹¤ìŒ ë‹¨ê³„:\n",
    "- `python simple_model_demo.py` ì‹¤í–‰í•˜ì—¬ ì¶”ê°€ í…ŒìŠ¤íŠ¸\n",
    "- ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ì‹¤ì œ ì‚¬ìš©ì í…ŒìŠ¤íŠ¸\n",
    "- ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ê°œì„ \n",
    "- ìƒˆë¡œìš´ ë°ì´í„°ë¡œ ëª¨ë¸ ì—…ë°ì´íŠ¸\n",
    "- ë‹¤ë¥¸ ì„ë² ë”© ë°©ë²• ì‹¤í—˜ (ì˜ˆ: BERT, Sentence-BERT)\n",
    "\n",
    "### ğŸ’¡ ê°œì„  ì•„ì´ë””ì–´:\n",
    "- ì‚¬ìš©ì í”¼ë“œë°±ì„ í†µí•œ ëª¨ë¸ ê°€ì¤‘ì¹˜ ì¡°ì •\n",
    "- ì‹œê°„ëŒ€ë³„, ê³„ì ˆë³„ ì¶”ì²œ ê°€ì¤‘ì¹˜ ì ìš©\n",
    "- ì‚¬ìš©ì ì„ í˜¸ë„ í•™ìŠµ ë° ê°œì¸í™”\n",
    "- ì‹¤ì‹œê°„ ì¸ê¸°ë„ ë°˜ì˜\n",
    "\n",
    "ğŸµ **ì´ì œ ë°±ì—”ë“œ ì„œë²„ë¥¼ ì‹¤í–‰í•˜ì—¬ ì›¹ì—ì„œ ì¶”ì²œ ì‹œìŠ¤í…œì„ ì²´í—˜í•´ë³´ì„¸ìš”!** ğŸµ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}