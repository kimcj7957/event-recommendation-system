{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# 🎵 이벤트 추천 시스템 - 완전한 모델 학습 데모\n",
    "\n",
    "이 노트북은 다중 모델 기반 이벤트 추천 시스템의 전체 학습 과정을 보여줍니다.\n",
    "\n",
    "## 📋 목차\n",
    "1. 환경 설정 및 데이터 로드\n",
    "2. 데이터 전처리 및 필터링\n",
    "3. TF-IDF 모델 학습\n",
    "4. LSA 모델 학습\n",
    "5. Word2Vec 대안 모델 학습\n",
    "6. 하이브리드 모델 구축\n",
    "7. 모델 성능 비교\n",
    "8. 추천 결과 시연"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 1. 환경 설정 및 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 머신러닝 라이브러리\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import hstack\n",
    "import re\n",
    "\n",
    "# 플롯 설정\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"🚀 모든 라이브러리가 성공적으로 로드되었습니다!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 모델에서 데이터 로드\n",
    "model_path = Path('../model/recommender_ko.joblib')\n",
    "\n",
    "if model_path.exists():\n",
    "    base_model = joblib.load(model_path)\n",
    "    df_original = base_model['df']\n",
    "    meta_preprocessor = base_model['pre']\n",
    "    print(f\"✅ 원본 데이터 로드 완료: {len(df_original):,}개 이벤트\")\n",
    "    print(f\"📈 데이터셋 크기: {df_original.shape}\")\n",
    "    print(f\"📋 컬럼: {list(df_original.columns)}\")\n",
    "else:\n",
    "    print(\"❌ 모델 파일을 찾을 수 없습니다. 먼저 백엔드를 실행해주세요.\")\n",
    "    df_original = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 2. 데이터 전처리 및 필터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 전처리 함수\n",
    "def preprocess_text(text):\n",
    "    \"\"\"텍스트 전처리 함수\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return ''\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', str(text).lower())\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def filter_valid_events(df):\n",
    "    \"\"\"유효한 이벤트 데이터만 필터링\"\"\"\n",
    "    print(\"🔧 데이터 필터링 및 전처리 중...\")\n",
    "    print(f\"원본 데이터: {len(df):,}개 이벤트\")\n",
    "    \n",
    "    # 기본 전처리\n",
    "    df_filtered = df.copy()\n",
    "    df_filtered['content_clean'] = df_filtered['content'].apply(preprocess_text)\n",
    "    df_filtered['place_clean'] = df_filtered['place'].apply(preprocess_text)\n",
    "    df_filtered['location_clean'] = df_filtered['loc_sigu'].apply(preprocess_text)\n",
    "    \n",
    "    # 필터링 조건\n",
    "    # 1. content가 비어있지 않고 10자 이상\n",
    "    valid_content = (df_filtered['content'].notna()) & (df_filtered['content'].str.len() >= 10)\n",
    "    print(f\"유효한 내용(10자 이상): {valid_content.sum():,}개\")\n",
    "    \n",
    "    # 2. place 정보가 있는 것\n",
    "    valid_place = df_filtered['place'].notna() & (df_filtered['place'].str.len() > 0)\n",
    "    print(f\"유효한 장소 정보: {valid_place.sum():,}개\")\n",
    "    \n",
    "    # 3. 가격 정보가 있는 것 (0보다 큰 값)\n",
    "    valid_price = df_filtered['price_adv'].notna() & (df_filtered['price_adv'] > 0)\n",
    "    print(f\"유효한 가격 정보: {valid_price.sum():,}개\")\n",
    "    \n",
    "    # 모든 조건을 만족하는 데이터만 선택\n",
    "    valid_mask = valid_content & valid_place & valid_price\n",
    "    df_filtered = df_filtered[valid_mask].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"✅ 필터링 완료: {len(df_filtered):,}개 이벤트 (제거된 것: {len(df) - len(df_filtered):,}개)\")\n",
    "    print(f\"📊 필터링 비율: {len(df_filtered)/len(df)*100:.1f}%\")\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "if df_original is not None:\n",
    "    # 데이터 필터링 적용\n",
    "    df = filter_valid_events(df_original)\n",
    "    \n",
    "    # 통합 텍스트 코퍼스 생성\n",
    "    text_corpus = (\n",
    "        df['content_clean'].fillna('') + ' ' +\n",
    "        df['place_clean'].fillna('') + ' ' +\n",
    "        df['location_clean'].fillna('')\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n📝 텍스트 코퍼스 통계:\")\n",
    "    print(f\"• 평균 길이: {text_corpus.str.len().mean():.0f}자\")\n",
    "    print(f\"• 최대 길이: {text_corpus.str.len().max():.0f}자\")\n",
    "    print(f\"• 최소 길이: {text_corpus.str.len().min():.0f}자\")\n",
    "    \n",
    "    # 샘플 데이터 확인\n",
    "    print(\"\\n🔍 필터링된 데이터 샘플:\")\n",
    "    display(df[['content', 'place', 'price_adv', 'loc_sigu']].head(3))\n",
    "else:\n",
    "    df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## 3. TF-IDF 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(\"🔤 TF-IDF 모델 학습 시작...\")\n",
    "    \n",
    "    # TF-IDF 벡터라이저 설정\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        max_features=10000,\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=2,\n",
    "        stop_words='english'\n",
    "    )\n",
    "    \n",
    "    # 텍스트 벡터화\n",
    "    X_text_tfidf = tfidf_vectorizer.fit_transform(text_corpus)\n",
    "    print(f\"✅ TF-IDF 벡터 생성: {X_text_tfidf.shape}\")\n",
    "    \n",
    "    # 메타데이터와 결합\n",
    "    X_meta = meta_preprocessor.transform(df)\n",
    "    X_combined_tfidf = hstack([X_text_tfidf, X_meta]).tocsr()\n",
    "    print(f\"✅ 통합 특성 벡터: {X_combined_tfidf.shape}\")\n",
    "    \n",
    "    # KNN 모델 학습\n",
    "    knn_tfidf = NearestNeighbors(metric='cosine', n_neighbors=20, n_jobs=-1)\n",
    "    knn_tfidf.fit(X_combined_tfidf)\n",
    "    print(\"✅ TF-IDF KNN 모델 학습 완료!\")\n",
    "    \n",
    "    # 상위 특성 분석\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    tfidf_scores = X_text_tfidf.mean(axis=0).A1\n",
    "    top_features = sorted(zip(feature_names, tfidf_scores), key=lambda x: x[1], reverse=True)[:10]\n",
    "    \n",
    "    print(\"\\n🏆 TF-IDF 상위 10개 특성:\")\n",
    "    for i, (feature, score) in enumerate(top_features, 1):\n",
    "        print(f\"{i:2d}. {feature:<15} (점수: {score:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 4. LSA (잠재 의미 분석) 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(\"🧮 LSA 모델 학습 시작...\")\n",
    "    \n",
    "    # Count Vectorizer (LSA에서 더 효과적)\n",
    "    count_vectorizer = CountVectorizer(\n",
    "        max_features=5000,\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=2,\n",
    "        stop_words='english'\n",
    "    )\n",
    "    \n",
    "    X_text_count = count_vectorizer.fit_transform(text_corpus)\n",
    "    print(f\"✅ Count 벡터 생성: {X_text_count.shape}\")\n",
    "    \n",
    "    # SVD로 차원 축소\n",
    "    svd = TruncatedSVD(n_components=100, random_state=42)\n",
    "    X_text_lsa = svd.fit_transform(X_text_count)\n",
    "    print(f\"✅ LSA 차원 축소: {X_text_lsa.shape}\")\n",
    "    print(f\"✅ 설명된 분산 비율: {svd.explained_variance_ratio_.sum():.3f}\")\n",
    "    \n",
    "    # 메타데이터와 결합\n",
    "    X_meta_dense = X_meta.toarray() if hasattr(X_meta, 'toarray') else X_meta\n",
    "    X_combined_lsa = np.hstack([X_text_lsa, X_meta_dense])\n",
    "    print(f\"✅ 통합 LSA 특성 벡터: {X_combined_lsa.shape}\")\n",
    "    \n",
    "    # KNN 모델 학습\n",
    "    knn_lsa = NearestNeighbors(metric='cosine', n_neighbors=20, n_jobs=-1)\n",
    "    knn_lsa.fit(X_combined_lsa)\n",
    "    print(\"✅ LSA KNN 모델 학습 완료!\")\n",
    "    \n",
    "    # LSA 성분 시각화\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, 101), svd.explained_variance_ratio_, 'b-', alpha=0.7)\n",
    "    plt.title('LSA 성분별 설명 분산')\n",
    "    plt.xlabel('성분 번호')\n",
    "    plt.ylabel('설명 분산 비율')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## 5. Word2Vec 대안 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(\"🔤 Word2Vec 대안 모델 학습 시작...\")\n",
    "    print(\"(HashingVectorizer를 사용하여 Word2Vec과 유사한 효과 구현)\")\n",
    "    \n",
    "    # HashingVectorizer로 Word2Vec 대안 구현\n",
    "    word2vec_hasher = HashingVectorizer(\n",
    "        n_features=1000,\n",
    "        ngram_range=(1, 3),\n",
    "        binary=False,\n",
    "        norm='l2',\n",
    "        lowercase=True,\n",
    "        stop_words='english'\n",
    "    )\n",
    "    \n",
    "    X_text_w2v = word2vec_hasher.fit_transform(text_corpus)\n",
    "    print(f\"✅ Word2Vec 대안 벡터 생성: {X_text_w2v.shape}\")\n",
    "    \n",
    "    # 메타데이터와 결합\n",
    "    X_combined_w2v = hstack([X_text_w2v, X_meta]).tocsr()\n",
    "    print(f\"✅ 통합 Word2Vec 특성 벡터: {X_combined_w2v.shape}\")\n",
    "    \n",
    "    # KNN 모델 학습\n",
    "    knn_w2v = NearestNeighbors(metric='cosine', n_neighbors=20, n_jobs=-1)\n",
    "    knn_w2v.fit(X_combined_w2v)\n",
    "    print(\"✅ Word2Vec 대안 KNN 모델 학습 완료!\")\n",
    "    \n",
    "    # 해싱 벡터 통계\n",
    "    hash_density = X_text_w2v.nnz / (X_text_w2v.shape[0] * X_text_w2v.shape[1])\n",
    "    print(f\"\\n📊 HashingVectorizer 통계:\")\n",
    "    print(f\"• 벡터 밀도: {hash_density:.4f}\")\n",
    "    print(f\"• 평균 비영 원소 수: {X_text_w2v.nnz / X_text_w2v.shape[0]:.1f}\")\n",
    "    print(f\"• 특성 차원: {X_text_w2v.shape[1]:,}개\")\n",
    "    \n",
    "    # Word2Vec 대안의 장점 설명\n",
    "    print(\"\\n💡 Word2Vec 대안의 특징:\")\n",
    "    print(\"• HashingVectorizer는 단어의 해시값을 이용하여 고정 크기 벡터 생성\")\n",
    "    print(\"• 메모리 효율적이며 새로운 단어에도 대응 가능\")\n",
    "    print(\"• n-gram을 통해 단어 순서와 문맥 정보 일부 보존\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## 6. 하이브리드 모델 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(\"🔄 하이브리드 모델 구축...\")\n",
    "    \n",
    "    # 모든 모델을 딕셔너리로 정리\n",
    "    models = {\n",
    "        'tfidf': {\n",
    "            'vectorizer': tfidf_vectorizer,\n",
    "            'knn': knn_tfidf,\n",
    "            'feature_matrix': X_combined_tfidf,\n",
    "            'description': 'TF-IDF 기반 키워드 매칭',\n",
    "            'type': 'sparse'\n",
    "        },\n",
    "        'lsa': {\n",
    "            'count_vectorizer': count_vectorizer,\n",
    "            'svd': svd,\n",
    "            'knn': knn_lsa,\n",
    "            'feature_matrix': X_combined_lsa,\n",
    "            'description': 'LSA 기반 잠재 의미 분석',\n",
    "            'type': 'dense'\n",
    "        },\n",
    "        'word2vec': {\n",
    "            'hasher': word2vec_hasher,\n",
    "            'knn': knn_w2v,\n",
    "            'feature_matrix': X_combined_w2v,\n",
    "            'description': 'HashingVectorizer 기반 단어 임베딩 대안',\n",
    "            'type': 'sparse'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"✅ 하이브리드 모델 준비 완료!\")\n",
    "    print(\"\\n🎯 학습된 모델들:\")\n",
    "    for name, model in models.items():\n",
    "        matrix_shape = model['feature_matrix'].shape\n",
    "        print(f\"• {name.upper()}: {model['description']}\")\n",
    "        print(f\"  - 특성 행렬 크기: {matrix_shape}\")\n",
    "        print(f\"  - 데이터 타입: {model['type']}\")\n",
    "        print()\n",
    "    \n",
    "    # 하이브리드 추천 함수 정의\n",
    "    def hybrid_recommend(query, top_k=5, weights=None):\n",
    "        \"\"\"하이브리드 추천: 여러 모델의 결과를 가중 평균\"\"\"\n",
    "        if weights is None:\n",
    "            weights = {'tfidf': 0.4, 'lsa': 0.3, 'word2vec': 0.3}\n",
    "        \n",
    "        all_results = []\n",
    "        \n",
    "        for model_name, weight in weights.items():\n",
    "            try:\n",
    "                q_vec = encode_query_for_model(query, model_name)\n",
    "                distances, indices = models[model_name]['knn'].kneighbors(q_vec, n_neighbors=top_k*2)\n",
    "                \n",
    "                for idx, dist in zip(indices[0], distances[0]):\n",
    "                    similarity = (1 - dist) * weight\n",
    "                    all_results.append((idx, similarity, model_name))\n",
    "            except Exception as e:\n",
    "                print(f\"Error in {model_name}: {e}\")\n",
    "        \n",
    "        # 결과 집계 및 정렬\n",
    "        result_dict = {}\n",
    "        for idx, sim, model in all_results:\n",
    "            if idx not in result_dict:\n",
    "                result_dict[idx] = {'total_similarity': 0, 'models': []}\n",
    "            result_dict[idx]['total_similarity'] += sim\n",
    "            result_dict[idx]['models'].append(model)\n",
    "        \n",
    "        # 상위 k개 선택\n",
    "        sorted_results = sorted(result_dict.items(), \n",
    "                               key=lambda x: x[1]['total_similarity'], \n",
    "                               reverse=True)[:top_k]\n",
    "        \n",
    "        return sorted_results\n",
    "    \n",
    "    print(\"💡 하이브리드 모델 특징:\")\n",
    "    print(\"• TF-IDF: 정확한 키워드 매칭 (가중치 40%)\")\n",
    "    print(\"• LSA: 의미적 유사성 분석 (가중치 30%)\")\n",
    "    print(\"• Word2Vec 대안: 단어 임베딩 효과 (가중치 30%)\")\n",
    "    print(\"• 각 모델의 강점을 결합하여 더 다양하고 정확한 추천 제공\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## 7. 모델 성능 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # 쿼리 인코딩 함수\n",
    "    def encode_query_for_model(query, model_name):\n",
    "        \"\"\"쿼리를 특정 모델용으로 인코딩\"\"\"\n",
    "        keywords = query.get('keywords', '')\n",
    "        price_max = query.get('price_max', 25000)\n",
    "        location = query.get('location', 'unknown')\n",
    "        \n",
    "        # 메타데이터 처리\n",
    "        meta_df = pd.DataFrame([{\n",
    "            'price_adv': price_max,\n",
    "            'price_door': price_max,\n",
    "            'loc_sigu': location\n",
    "        }])\n",
    "        meta_vec = meta_preprocessor.transform(meta_df)\n",
    "        \n",
    "        model = models[model_name]\n",
    "        \n",
    "        if model_name == 'tfidf':\n",
    "            text_vec = model['vectorizer'].transform([keywords])\n",
    "            return hstack([text_vec, meta_vec])\n",
    "        elif model_name == 'lsa':\n",
    "            count_vec = model['count_vectorizer'].transform([keywords])\n",
    "            text_reduced = model['svd'].transform(count_vec)\n",
    "            meta_vec_dense = meta_vec.toarray() if hasattr(meta_vec, 'toarray') else meta_vec\n",
    "            return np.hstack([text_reduced, meta_vec_dense])\n",
    "        elif model_name == 'word2vec':\n",
    "            text_vec = model['hasher'].transform([keywords])\n",
    "            return hstack([text_vec, meta_vec])\n",
    "    \n",
    "    # 테스트 쿼리 정의\n",
    "    test_queries = [\n",
    "        {\"keywords\": \"재즈 콘서트\", \"price_max\": 50000, \"location\": \"강남구\"},\n",
    "        {\"keywords\": \"클래식 음악회\", \"price_max\": 30000, \"location\": \"종로구\"},\n",
    "        {\"keywords\": \"록 페스티벌\", \"price_max\": 80000, \"location\": \"마포구\"},\n",
    "        {\"keywords\": \"팝 공연\", \"price_max\": 40000, \"location\": \"서초구\"}\n",
    "    ]\n",
    "    \n",
    "    print(\"📊 모델 성능 비교 테스트\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 각 모델의 성능 측정\n",
    "    performance_results = {}\n",
    "    \n",
    "    for model_name in models.keys():\n",
    "        print(f\"\\n🔍 {model_name.upper()} 모델 테스트:\")\n",
    "        model_results = []\n",
    "        \n",
    "        for i, query in enumerate(test_queries, 1):\n",
    "            try:\n",
    "                q_vec = encode_query_for_model(query, model_name)\n",
    "                distances, indices = models[model_name]['knn'].kneighbors(q_vec, n_neighbors=5)\n",
    "                similarities = 1 - distances[0]\n",
    "                avg_similarity = similarities.mean()\n",
    "                model_results.append(avg_similarity)\n",
    "                print(f\"  쿼리 {i}: '{query['keywords']}' → 평균 유사도: {avg_similarity:.3f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  쿼리 {i} 오류: {e}\")\n",
    "                model_results.append(0)\n",
    "        \n",
    "        performance_results[model_name] = model_results\n",
    "    \n",
    "    # 성능 요약\n",
    "    print(\"\\n📈 모델별 평균 성능:\")\n",
    "    print(\"-\" * 40)\n",
    "    for model_name, results in performance_results.items():\n",
    "        avg_performance = np.mean(results)\n",
    "        print(f\"• {model_name.upper():<10}: {avg_performance:.3f}\")\n",
    "    \n",
    "    # 성능 시각화\n",
    "    if len(performance_results) > 0:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # 쿼리별 성능 비교\n",
    "        plt.subplot(1, 2, 1)\n",
    "        x = np.arange(len(test_queries))\n",
    "        width = 0.25\n",
    "        \n",
    "        for i, (model_name, results) in enumerate(performance_results.items()):\n",
    "            plt.bar(x + i*width, results, width, label=model_name.upper(), alpha=0.8)\n",
    "        \n",
    "        plt.xlabel('테스트 쿼리')\n",
    "        plt.ylabel('평균 코사인 유사도')\n",
    "        plt.title('쿼리별 모델 성능 비교')\n",
    "        plt.xticks(x + width, [f\"Q{i+1}\" for i in range(len(test_queries))])\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 모델별 평균 성능\n",
    "        plt.subplot(1, 2, 2)\n",
    "        model_names = list(performance_results.keys())\n",
    "        avg_scores = [np.mean(performance_results[name]) for name in model_names]\n",
    "        \n",
    "        bars = plt.bar(model_names, avg_scores, color=['skyblue', 'lightgreen', 'orange'], alpha=0.8)\n",
    "        plt.ylabel('평균 유사도')\n",
    "        plt.title('모델별 전체 평균 성능')\n",
    "        plt.ylim(0, max(avg_scores) * 1.1)\n",
    "        \n",
    "        # 막대 위에 수치 표시\n",
    "        for bar, score in zip(bars, avg_scores):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                    f'{score:.3f}', ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## 8. 추천 결과 시연"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    def get_recommendations(query, model_name, top_k=3):\n",
    "        \"\"\"특정 모델로 추천 결과 반환\"\"\"\n",
    "        try:\n",
    "            q_vec = encode_query_for_model(query, model_name)\n",
    "            distances, indices = models[model_name]['knn'].kneighbors(q_vec, n_neighbors=top_k)\n",
    "            \n",
    "            results = df.iloc[indices[0]].copy()\n",
    "            results['similarity'] = 1 - distances[0]\n",
    "            \n",
    "            return results[['content', 'place', 'price_adv', 'loc_sigu', 'similarity']]\n",
    "        except Exception as e:\n",
    "            print(f\"추천 오류 ({model_name}): {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    # 시연용 쿼리들\n",
    "    demo_queries = [\n",
    "        {\n",
    "            \"keywords\": \"재즈 콘서트 라이브\",\n",
    "            \"price_max\": 60000,\n",
    "            \"location\": \"강남구\"\n",
    "        },\n",
    "        {\n",
    "            \"keywords\": \"클래식 오케스트라\",\n",
    "            \"price_max\": 50000,\n",
    "            \"location\": \"종로구\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for query_idx, demo_query in enumerate(demo_queries, 1):\n",
    "        print(f\"\\n🎯 데모 쿼리 {query_idx}: {demo_query}\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # 각 모델별 추천 결과 비교\n",
    "        for model_name in models.keys():\n",
    "            print(f\"\\n🤖 {model_name.upper()} 모델 추천 결과:\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            recommendations = get_recommendations(demo_query, model_name, top_k=3)\n",
    "            \n",
    "            if not recommendations.empty:\n",
    "                for i, (idx, row) in enumerate(recommendations.iterrows(), 1):\n",
    "                    print(f\"{i}. 📍 {row['place'][:40]}... ({row['loc_sigu']})\")\n",
    "                    print(f\"   💰 가격: {row['price_adv']:,.0f}원\")\n",
    "                    print(f\"   📊 유사도: {row['similarity']:.3f}\")\n",
    "                    print(f\"   📝 내용: {row['content'][:80]}...\")\n",
    "                    print()\n",
    "            else:\n",
    "                print(\"❌ 추천 결과를 가져올 수 없습니다.\")\n",
    "        \n",
    "        # 하이브리드 추천 시연\n",
    "        print(f\"\\n🔄 HYBRID 모델 추천 결과 (가중 평균):\")\n",
    "        print(\"-\" * 50)\n",
    "        try:\n",
    "            hybrid_results = hybrid_recommend(demo_query, top_k=3)\n",
    "            for i, (idx, result_info) in enumerate(hybrid_results, 1):\n",
    "                event = df.iloc[idx]\n",
    "                print(f\"{i}. 📍 {event['place'][:40]}... ({event['loc_sigu']})\")\n",
    "                print(f\"   💰 가격: {event['price_adv']:,.0f}원\")\n",
    "                print(f\"   📊 종합 점수: {result_info['total_similarity']:.3f}\")\n",
    "                print(f\"   🤝 참여 모델: {', '.join(result_info['models'])}\")\n",
    "                print(f\"   📝 내용: {event['content'][:80]}...\")\n",
    "                print()\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 하이브리드 추천 오류: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## 🎉 학습 완료!\n",
    "\n",
    "축하합니다! 다중 모델 이벤트 추천 시스템의 전체 학습 과정이 완료되었습니다.\n",
    "\n",
    "### ✅ 구현된 기능들:\n",
    "\n",
    "#### 1. 데이터 전처리 및 필터링\n",
    "- 본문이 10자 이하인 이벤트 제외\n",
    "- 장소, 가격 정보가 없는 이벤트 제외\n",
    "- 텍스트 정규화 및 정제\n",
    "\n",
    "#### 2. 학습된 모델들\n",
    "- **TF-IDF 모델**: 키워드 기반 정확한 매칭\n",
    "- **LSA 모델**: 잠재 의미 분석으로 의미적 유사성 고려  \n",
    "- **Word2Vec 대안**: HashingVectorizer로 단어 임베딩 효과\n",
    "- **하이브리드 모델**: 세 모델의 가중 평균으로 종합적 추천\n",
    "\n",
    "#### 3. 성능 평가\n",
    "- 다양한 쿼리로 모델별 성능 비교\n",
    "- 시각화를 통한 성능 분석\n",
    "- 코사인 유사도 기반 정량적 평가\n",
    "\n",
    "#### 4. 실제 추천 시연\n",
    "- 각 모델별 추천 결과 비교\n",
    "- 하이브리드 모델의 종합적 추천\n",
    "- 유사도 점수 및 참여 모델 표시\n",
    "\n",
    "### 🚀 다음 단계:\n",
    "- `python simple_model_demo.py` 실행하여 추가 테스트\n",
    "- 웹 애플리케이션에서 실제 사용자 테스트\n",
    "- 성능 모니터링 및 개선\n",
    "- 새로운 데이터로 모델 업데이트\n",
    "- 다른 임베딩 방법 실험 (예: BERT, Sentence-BERT)\n",
    "\n",
    "### 💡 개선 아이디어:\n",
    "- 사용자 피드백을 통한 모델 가중치 조정\n",
    "- 시간대별, 계절별 추천 가중치 적용\n",
    "- 사용자 선호도 학습 및 개인화\n",
    "- 실시간 인기도 반영\n",
    "\n",
    "🎵 **이제 백엔드 서버를 실행하여 웹에서 추천 시스템을 체험해보세요!** 🎵"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}